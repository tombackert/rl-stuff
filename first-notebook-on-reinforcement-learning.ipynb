{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4v/5gdupoY6EeEsjGtn0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tombackert/rl-stuff/blob/main/first-notebook-on-reinforcement-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Notebook on Reinforcement Learning"
      ],
      "metadata": {
        "id": "M6f0S0mJyG6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Solving the Lunar Lander Problem\n",
        "\n",
        "Sources:\n",
        "1. [Gymnasium](https://gymnasium.farama.org/)\n",
        "2. [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/#)\n",
        "3. [Deep Mind paper](https://arxiv.org/abs/1312.5602)\n",
        "4. [Natur article](https://www.nature.com/articles/nature14236)\n",
        "5. [Tuned Hyperparamters for DQN Model](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml)\n",
        "\n",
        "Reinforcement Learning Resources:\n",
        "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)\n",
        "- [Huggingface's Deep RL Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)\n",
        "- [Lilian Weng's Blog](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)\n",
        "- [Berkley's Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n",
        "\n",
        "\n",
        "*more updates coming...*"
      ],
      "metadata": {
        "id": "wGF5YCC5wwfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Dependencies and Imports"
      ],
      "metadata": {
        "id": "MyBN_3kB-Mlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dependencies\n",
        "!apt-get update && apt-get install swig cmake\n",
        "!pip install box2d-py\n",
        "\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ],
      "metadata": {
        "id": "83kjAfMOqtvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import gymnasium as gym\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "metadata": {
        "id": "zNMvOJNr-FrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Building and Training the Agent"
      ],
      "metadata": {
        "id": "_e7Vt5MD-JAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep RL Model using the DQN Algorithm\n",
        "\n",
        "# hyperparameters (tuned from link 5)\n",
        "policy=\"MlpPolicy\"\n",
        "env=\"LunarLander-v2\"\n",
        "learning_rate=6.3e-4\n",
        "buffer_size=50000\n",
        "learning_starts=0\n",
        "batch_size=128\n",
        "target_update_interval=250\n",
        "gamma=0.99\n",
        "train_freq=4\n",
        "gradient_steps=-1\n",
        "exploration_fraction=0.12\n",
        "exploration_final_eps=0.1\n",
        "policy_kwargs=dict(net_arch=[256, 256])\n",
        "\n",
        "\n",
        "# model\n",
        "model = DQN(policy=policy,\n",
        "            env=env,\n",
        "            learning_rate=learning_rate,\n",
        "            buffer_size=buffer_size,\n",
        "            learning_starts=learning_starts,\n",
        "            batch_size=batch_size,\n",
        "            target_update_interval=target_update_interval,\n",
        "            gamma=gamma,\n",
        "            train_freq=train_freq,\n",
        "            gradient_steps=gradient_steps,\n",
        "            exploration_fraction=exploration_fraction,\n",
        "            exploration_final_eps=exploration_final_eps,\n",
        "            policy_kwargs=policy_kwargs\n",
        "            ).learn(total_timesteps=100_000, progress_bar=True)\n",
        "\n",
        "# trains aproxi 8 mins on gpu"
      ],
      "metadata": {
        "id": "AWyRhqOwuafi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Testing the Agent"
      ],
      "metadata": {
        "id": "qbeaItwh-ZN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a gif\n",
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render(mode=\"rgb_array\")\n",
        "for i in range(5000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = model.env.step(action)\n",
        "    img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "imageio.mimsave(\"lander_dqn.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ],
      "metadata": {
        "id": "yDCVFW8OuCHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(mean_reward, std_reward) # 245.4761378 72.87695287287264"
      ],
      "metadata": {
        "id": "wzVoUEgqwXLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}